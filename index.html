<!doctype html>
<meta charset="utf-8">
<script src="https://distill.pub/template.v1.js"></script>

<script type="text/front-matter">
  title: "Multimodal Compact Bilinear Pooling in VQA"
  description: "Description of the post"
  authors:
  - Bosen Ding:
  - Wenjing Kang:
  affiliations:
  - SHAOCONG LIU:
  - KAREN LU:
</script>

<dt-article class="centered">
  <h1>Multimodal Compact Bilinear Pooling in VQA</h1>
  <h2>University of California, at Berkeley</h2>
  <dt-byline></dt-byline>

<p>In this blog, we will discuss a re-implementation attempt of MCB VQA in tensorflow. The accuracy we get is about 30 %, which is lower than the 60% from the original paper. Corresponding lessons are discussed in detail in result and lesson sections. 
  </p>

  <h2>Problem Statement and Background</h2>
  <p>In this project, our group focused on the problem of Visual Question Answer <dt-cite key="1"></dt-cite>, the intersection of computer vision and natural language processing. Naturally, we humans associate visual stimuli with symbols defined within our language.  In addition to captioning and assigning each scene with corresponding description, we also use our language to reason with the information presented by the visual stimuli.  To mimic this task, our neural network requires, in addition to image captioning techniques, a direct emphasis on associating the question answering process with the presented visual informations. The completion of this task will open up a new world to the application of artificial intelligence. For example, instead of only asking Alexa ”What is the weather today?”, people will be able to show Alexa an image of Trump and ask ”How do you like his hair?” 
  </p>

<div>
<div class="images">
  <img src="http://visualqa.org/static/img/challenge.png" height="200" width="600">
  <!-- <p><font size="3"> by Edward Munch. Painted in 1893. </font></p> -->
 </div>
   <figcaption> <font size="2"> Figure 1, http://visualqa.org/challenge.html</font></figcaption>
 </div>
    
<p>
We used the VQA challenge data, which can be downloaded from http://visualqa.org/index.html. We used the version 1 data set which contains roughly 250,000 questions and 82,000 images for training and another half-sized data for validation. The image data is about 20 GB. <br /> <br />

Currently, there are a few state-of-the-art solutions that address the problem of finding the correspondence between vision and natural language embeddings. Concatenating the two vectors of visual and language embeddings is one way; elementwise multiplication is another. The most recent and most novel method, however, is one that uses lower dimension approximation of the outer product. “Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding”<dt-cite key="2"></dt-cite> proposes the method of Multimodal Compact Bilinear pooling that will approximate outer product of the visual embedding vector and the textual embedding vector. <br /> <br />

Another method is proposed in the paper ”Hierarchical Question-Image Co-Attention for Visual Question Answering” <dt-cite key="3"></dt-cite>. The authors introduces the concept of attention, which indicates the importance of each part of the picture to the given question or vice versa. The concept of attention can be captured with a weight matrix. In the case of co-attention, the weight matrix is used in a bilinear mapping and hence weights the same part of question and picture vectors. In the alternating co-attention case, each modality is weighted separately, but each of them provides an weighted offset to the other modality.
</p>

<h2>Approach</h2>

<p>
  Our approach started with preprocessing our data using resnet and lstm for the image and textual parts respectively. With MSCOCO VQA dataset, we first performed feature extraction on the images using resnet-152. This part of the preprocessing utilized pretrained resnet-152 model and stored the last fully-connected layer’s output for each image as the feature of the image. Each image, after this process, is represented by a 2048 x 14 x 14 matrix that will be fed into our model. This part is done using caffe on AWS. <br /> <br />
For the textual information, we first made dictionaries of all the words appeared in questions and answers. Then, we tokenized each question sentence based on the vocabulary into vector in all of our four models, two of which used Glove for extra textual representation. Then we extracted featury from a word embedding matrix and then feeded the vector into a two-layer LSTM. We limited the maximum number of words in a question as 15. Therefore we used the output of the two-layer LSTM after feeding 15 times as the representation of this question. Each output of LSTM is 1024 size, so the question output size is 2048. When attention is used, we tiled the question vector into 2048 x 14 x 14 to match the size of image feature. 
When Glove was used, we combined the word-embedding vector and the corresponding Glove embedding before feeding into the LSTM network. 
</p>

<div class="image">
  <img src="https://raw.githubusercontent.com/shaocongliu/MCB_in_VQA/master/plots/mcb1.png" height="180" width="650">
 </div>

<p>

Our basic model uses the image and textual(without Glove) input as mentioned above and then feeds both features into the multimodal compact bilinear <dt-cite key="5"></dt-cite> layer and then a fully connected layer for softmax prediction. According to the VQA paper <dt-cite key="2"></dt-cite>, the most frequent 3000 answers are used as a classification problem, which theoretically contain about 85% of correct answer set.
<br /> <br /> 
The original paper <dt-cite key="2"></dt-cite> did not implement the MCB themselves, so we just used a tensorflow MCB CUDA implementation <dt-cite key="4"></dt-cite> we found online. 
</p>
<div class="image">
  <img src="https://raw.githubusercontent.com/shaocongliu/MCB_in_VQA/master/plots/mcb2.png" height="230" width="700">
 </div>

<p>
We also implemented the attention as introduced in the paper <dt-cite key="2"></dt-cite>, which would improve the accuracy of the model. Instead of a multi-layer attention as completed in the paper, we only used the vanila attention mechanism in our model.  
</p>

  <h2>Results</h2>
<p>
  We train our model on the VQA training split and test on validation split, as the original challenge proposes. We validate our ongoing training model every 1000 iteration on a random selected batch, which consists of 32 image and question pairs, from validation set. Despite our continuous efforts, our results are still disappointingly below our expectation. As we try to reimplement the state-of-the-art method of VQA, our accuracy remains below the expected outcome.  <br /> <br />
The results for baseline model, baseline mcb with glove, attention mcb, and our final model of attention mcb with glove are shown in the following plots.  Our training accuracy stuck around 25% while our validation accuracy stuck around 30 %.
</p>

<div align="center">
  <table style="width=100%">
    <tr>
        <img src="https://raw.githubusercontent.com/shaocongliu/MCB_in_VQA/master/plots/train_average_accuracy.jpg" align="middle" height="480" width="640">
        <figcaption align="middle">training accuracy</figcaption>
    </tr>
  </table>
</div>
<div align="center">
  <table style="width=100%">
    <tr>
        <img src="https://raw.githubusercontent.com/shaocongliu/MCB_in_VQA/master/plots/train_average_loss.jpg" align="middle" height="480" width="640">
        <figcaption align="middle">training loss</figcaption>
    </tr>
    </tr>
  </table>
</div>
<div align="center">
  <table style="width=100%">
    <tr>
        <img src="https://raw.githubusercontent.com/shaocongliu/MCB_in_VQA/master/plots/val_accuracy.jpg" align="middle" height="480" width="640">
        <figcaption align="middle">validation accuracy</figcaption>
    </tr>
    </tr>
  </table>
</div>
<div align="center">
  <table style="width=100%">
    <tr>
        <img src="https://raw.githubusercontent.com/shaocongliu/MCB_in_VQA/master/plots/val_loss.jpg" align="middle" height="480" width="640">
        <figcaption align="middle">validation loss</figcaption>
    </tr>
    </tr>
  </table>
</div>

<p>
  Even though our training loss kept decreasing, our training/validating accuracy started to lose major increase not long after 10,000~20,000 iterations. We checked again and again the correctness of our model by running mini epoches to see if our deep network was able to overfit. Our model passed every check by overfitting quickly within tens of iterations. Yet the validation results kept showing that there were mis-implementations in our model. <br /> <br />
Therefore, we went back to check the original data source and our validation method. By running a validation batch on our 75,000 iteration models, we found that our model tended to predict “yes” for most questions, regardless of the question type. We started to wonder that was there a data skew in the original questions’ composition. And indeed, by researching into the answers’ distribution, we found that “yes” appears to be the most popular answer for major question types such us “does…?” and “is there…?” Consequently, our model tended to learn this pattern rather than the real logic behind reasoning around the question by “looking at” the image. With the prevailing majority of the “yes” answer, the original paper might have re-balanced the data during its training to focus on those questions without the “yes” answer.<br /> <br />
The original paper <dt-cite key="2"></dt-cite> did not present a learning rate schedule, so the paper suggests the learning rate stays constant during the whole training process. The model thus might just chase each training batch to overfit that single batch, so it ended up not doing well. 
</p>

<h2>Tools</h2>
<p>
The pre-processing of images is completed in caffe on AWS. While the model development and debugging is mostly done locally, the actual training and validation of the model is completed in tensorflow on AWS. We used AWS p2 instance equipped with a NVIDIA K80 GPU.
</p>

<h2>Lessons Learned</h2>
<p>
As discussed above in the result section, we believe our major problem results from the imbalance of the data and the parameter tuning.  Should time permit, we would analyze the answer distribution more carefully, possible re-balance the data set to have a more meaning representation of the answer set. In addition, we could explore more parameters, especially learning rate and dropout rate.
</p>

<h2>Team Contributions</h2>
<p>
Bosen Ding: implemented the neural network structures, debugged the networks, modified the data provider code from official Visual API and wrote the report. (25%)</p><p>
Shaocong Liu: Implemented the training code, ran the code on AWS, debugged and wrote the slides, poster, and report. (25%)</p><p>
Wenjing Kang: pre-processed images in Caffe, set up AWS environment, debugged, ran the code on AWS  and wrote the report. (25%)</p><p>
Karen Lu:  Wrote data extraction without VQA official API, Tenforflow resnet weight initialization, implemented lstm code. (25%)
</p>

<h2>Codes</h2>
<p>Our codes were uploaded to github repo: <a href="https://github.com/shaocongliu/MCB_in_VQA">MCB_in_VQA.</a></p>


</dt-article>


<dt-appendix>

</dt-appendix>

<script type="text/bibliography">
  @article{1,
    title={VQA: Visual Question Answering},
    author={Aishwarya, Agrawal and Jiasen, Lu and Stanisla, Antol and Margaret, Mitchell and C. Lawrence, Zitnick and Dhruv, Batra and Devi, Parikh},
    journal={arXivreprint arXiv:1505.00468},
    year={2015},
    url={https://arxiv.org/pdf/1505.00468.pdf}
  }
  @article{2,
    title={Multimodal Compact Bilinear Pooling for Visual Question Answering and Visual Grounding},
    author={Akira, Fukui and Dong Huk, Park and Daylen, Yang and Anna, Rohrbach and Trevor, Darrell and Marcus, Rohrbach},
    journal={arXivreprint arXiv:1606.01847},
    year={2016},
    url={https://arxiv.org/pdf/1606.01847.pdf}
  }
  @article{3,
    title={Hierarchical Question-Image Co-Attention for Visual Question Answering},
    author={Jiasen, Lu and Jianwei, Yang and Dhruv, Batra and Devi, Parikh},
    journal={arXivreprint arXiv:1606.00061},
    year={2015},
    url={https://arxiv.org/pdf/1606.00061.pdf}
  }
  @article{4,
    title={tensorflow_compact_bilinear_pooling},
    author={Ronghang, Hu},
    journal={Github ronghanghu},
    year={2017},
    url={https://github.com/ronghanghu/tensorflow_compact_bilinear_pooling}
  }
  @article{5,
    title={Compact Bilinear Pooling},
    author={Yang, Gao and Oscar, Beijbom and Ning, Zhang and Trevor, Darrell},
    journal={arXivreprint arXiv:1511.06062},
    year={2016},
    url={https://arxiv.org/pdf/1511.06062.pdf}
  }
</script>

